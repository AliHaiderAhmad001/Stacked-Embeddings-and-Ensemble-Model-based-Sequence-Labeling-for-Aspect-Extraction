# -*- coding: utf-8 -*-
"""stack_re16_emb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J2spATWXxkM2uPo_Fh5zsBG94XY91HqL
"""

!pip install transformers
!pip install flair

# !pip install transformers
# !pip install flair
import warnings
warnings.filterwarnings("ignore")
import sys
sys.path.append('/content/drive/MyDrive/Colab Notebooks/AE/embeddings/generate_emb')
import numpy as np
import json
from bert__features import BERTModelFeatures
from elmo_features import ELMoModelFeatures
from transformers import BertTokenizer, BertModel
from deberta_features import transformerEmb

#load data 
def load__data(file_path):
    data,labels,words,tags = [],[],[],[]
    tag2idx={'B-A': 1, 'I-A': 2, 'O': 0}
    fh = open(file_path)
    for line in fh:
        line = line.strip()
        if line=='':
            #Sentence ended.
            if len(tags) <70:
              tags+=[0]*(70-len(tags))
            data.append(words)
            labels.append(tags)
            words,tags = [],[]
        else:
            word, tag = line.split("\t")
            words.append(word)
            tags.append(tag2idx[tag])
    fh.close()
    return data,labels

## generate emb for training
def stacked_trining_emb(data,se_path,fastText=False):
    idx=[]
    j=0
    for i,sent in enumerate(data):
      sent=" ".join(sent)
      v1=bf.tokenVecSum(sent)['token_vecs_sum']
      v2=ef.elmo_vectors([sent]).numpy()
      v4=tr.getEmb(sent)
      if v2.ndim==1:
        v2=v2.reshape(1,-1)   
      if fastText:
        v3=[]
        for token in sent.split():
          v3.append(in_domain_emb[word_idx[token]])           
        v=np.concatenate((v1,v2,v3,v4), axis=-1)
      else:
        try:           
          v=np.concatenate((v1,v2,v4), axis=-1)
        except:
          idx.append(i)
          continue
      if i%500==0:
        print(i)
        print(v.shape)
      np.save(se_path.format(j)+'.npy',v)
      j+=1
    return idx

# .. for testing
def stacked_test_emb(data,se_path,n_features=2816,max_line=70,fastText=False):
    all=[]
    idx=[]
    pad=np.zeros(n_features ,dtype='float32')
    for i,sent in enumerate(data):
      sent=" ".join(sent)
      v1=bf.tokenVecSum(sent)['token_vecs_sum']
      v2=ef.elmo_vectors([sent]).numpy()
      if v2.ndim==1:
        v2=v2.reshape(1,-1)
      v4=tr.getEmb(sent) 
      if fastText:
        v3=[]
        for token in sent.split():
          v3.append(in_domain_emb[word_idx[token]])           
        v=np.concatenate((v1,v2,v3,v4), axis=-1)
      else:
        try:           
          v=np.concatenate((v1,v2,v4), axis=-1)
        except:
          idx.append(i)
          continue
      if v.shape[0] !=max_line:
        v=np.concatenate((v,[pad]*(max_line-v.shape[0])), axis=0)
      all.append(v)
    all=np.array(all)
    print(all.shape)
    np.save(se_path,all)
    return idx

# load models
bf=BERTModelFeatures() # out of domain
ef=ELMoModelFeatures() # out of domain
tr=transformerEmb(model='yangheng/deberta-v3-large-absa-v1.1') # in domain Re
# fast text models
out_fn='/content/drive/MyDrive/Colab Notebooks/AE/embeddings/fast text_emb/output/final_indomain_emb.npy'
word_idx_fn='/content/drive/MyDrive/Colab Notebooks/AE/embeddings/fast text_emb/output/worIdx_.json'
with open(word_idx_fn) as f:
    word_idx=json.load(f)
in_domain_emb=np.load(out_fn)

########################## Re14-Datasets ###########################
# load IOB dataset
train_path = '/content/drive/MyDrive/Colab Notebooks/AE/AE_Datasets/Restaurants_Train_v2_mod.iob'
test_path = '/content/drive/MyDrive/Colab Notebooks/AE/AE_Datasets/Restaurants_Test_Gold_mod.iob'
train,train_labels = load__data(train_path)
test,test_labels = load__data(test_path)

training_data_path='/content/drive/MyDrive/Colab Notebooks/AE/embeddings/stacked2/train/id-{}'
idx1=stacked_trining_emb(train,training_data_path)

test_data_path='/content/drive/MyDrive/Colab Notebooks/AE/embeddings/stacked2/test/test_data.npy'
idx2=stacked_test_emb(test,test_data_path,n_features=2916,max_line=70)

# Save labels
training_labels_path='/content/drive/MyDrive/Colab Notebooks/AE/embeddings/Re14Labels/training_labels.npy'
test_labels_path='/content/drive/MyDrive/Colab Notebooks/AE/embeddings/Re14Labels/test_labels.npy'
np.save(training_labels_path,[x for i,x in enumerate(train_labels) if i not in idx1])
np.save(test_labels_path,[x for i,x in enumerate(test_labels) if i not in idx2])
print(len(idx1),len(idx2))

########################## Re16-Datasets ###########################
# load IOB dataset
train_path = '/content/drive/MyDrive/Colab Notebooks/AE/AE_Datasets/ABSA16_Restaurants_Train_SB1_v2_mod.iob'
test_path = '/content/drive/MyDrive/Colab Notebooks/AE/AE_Datasets/EN_REST_SB1_TEST_mod.iob'
train,train_labels = load__data(train_path)
test,test_labels = load__data(test_path)


training_data_path='/content/drive/MyDrive/Colab Notebooks/AE/embeddings/Re16DataEmb/train/id-{}'
idx1=stacked_trining_emb(train,training_data_path)

test_data_path='/content/drive/MyDrive/Colab Notebooks/AE/embeddings/Re16DataEmb/test/test_data.npy'
idx2=stacked_test_emb(test,test_data_path,n_features=2816,max_line=70)

# Save labels
training_labels_path='/content/drive/MyDrive/Colab Notebooks/AE/embeddings/Re16Labels/training_labels.npy'
test_labels_path='/content/drive/MyDrive/Colab Notebooks/AE/embeddings/Re16Labels/test_labels.npy'
np.save(training_labels_path,[x for i,x in enumerate(train_labels) if i not in idx1])
np.save(test_labels_path,[x for i,x in enumerate(test_labels) if i not in idx2])
print(len(idx1),len(idx2))

########################## La14-Datasets ###########################
tr=transformerEmb(model='yangheng/deberta-v3-large-absa-v1.1') # # in domain La

# load IOB dataset
train_path = '/content/drive/MyDrive/Colab Notebooks/AE/AE_Datasets/Laptop_Train_v2_mod.iob'
test_path = '/content/drive/MyDrive/Colab Notebooks/AE/AE_Datasets/Laptops_Test_Gold_mod.iob'
train,train_labels = load__data(train_path)
test,test_labels = load__data(test_path)

training_data_path='/content/drive/MyDrive/Colab Notebooks/AE/embeddings/LaptopDataEmb/train/id-{}'
idx1=stacked_trining_emb(train,training_data_path)

test_data_path='/content/drive/MyDrive/Colab Notebooks/AE/embeddings/LaptopDataEmb/test/test_data.npy'
idx2=stacked_test_emb(test,test_data_path,n_features=2816,max_line=80)

# Save labels
training_labels_path='/content/drive/MyDrive/Colab Notebooks/AE/embeddings/LaptopDataLabels/training_labels.npy'
test_labels_path='/content/drive/MyDrive/Colab Notebooks/AE/embeddings/LaptopDataLabels/test_labels.npy'
np.save(training_labels_path,[x for i,x in enumerate(train_labels) if i not in idx1])
np.save(test_labels_path,[x for i,x in enumerate(test_labels) if i not in idx2])
print(len(idx1),len(idx2))